"""
Feature-LSTMæ¨¡å‹ - æ—¶åºç‰ˆæœ¬ï¼ˆå¢é‡é¢„æµ‹ï¼‰
# æ¢ç´¢dui
è¾“å…¥: æ—¶åºè§¦è§‰æ•°æ® forces_l[seq_len, 3, 20, 20] + forces_r[seq_len, 3, 20, 20] -> CNNç‰¹å¾æå– -> LSTM -> é¢„æµ‹åŠ¨ä½œå¢é‡
è¾“å‡º: action_delta[3] = 3ç»´åŠ¨ä½œå¢é‡ï¼Œæœ€ç»ˆè¾“å‡º = current_action + action_delta
æ³¨æ„: 
1. ç½‘ç»œé¢„æµ‹å¢é‡è€Œéç»å¯¹åŠ¨ä½œï¼Œæé«˜é¢„æµ‹ç¨³å®šæ€§
2. æŸå¤±è®¡ç®—ä½¿ç”¨è½¬æ¢åçš„ç»å¯¹é¢„æµ‹ï¼Œä¿æŒç°æœ‰é€»è¾‘ä¸å˜
3. æ”¯æŒå¯†é›†ç›‘ç£æ¨¡å¼çš„ç´¯ç§¯å¢é‡é¢„æµ‹
"""

import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# å¯¼å…¥CNNè‡ªç¼–ç å™¨
try:
    from haptic.models.cnn_ae.cnn_autoencoder import TactileCNNAutoencoder
except ImportError:
    # å¼€å‘ç¯å¢ƒä¸‹çš„å¯¼å…¥
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    sys.path.insert(0, project_root)
    from cnn_ae.cnn_autoencoder import TactileCNNAutoencoder



class TactilePolicyFeatureLSTM(nn.Module):
    """
    è§¦è§‰ç­–ç•¥Feature-LSTMæ¨¡å‹ - æ—¶åºé¢„æµ‹ç‰ˆæœ¬ï¼ˆå¢é‡é¢„æµ‹ï¼‰
    åŸºäºé¢„è®­ç»ƒè§¦è§‰ç‰¹å¾çš„æ—¶åºå¢é‡é¢„æµ‹
    
    æ¶æ„ï¼š
    1. é¢„è®­ç»ƒCNNç¼–ç å™¨æå–å·¦å³æ‰‹è§¦è§‰ç‰¹å¾ (128ç»´ Ã— 2 = 256ç»´)
    2. åŠ¨ä½œåµŒå…¥ç¼–ç å™¨ (3 â†’ action_embed_dim)
    3. æ‹¼æ¥ç‰¹å¾å’ŒåŠ¨ä½œ (256 + action_embed_dim)
    4. é€šè¿‡LSTMå¤„ç†æ—¶åºä¿¡æ¯
    5. å…¨è¿æ¥å±‚é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»åŠ¨ä½œå¢é‡
    6. å¢é‡ + å½“å‰åŠ¨ä½œ = ç»å¯¹é¢„æµ‹ï¼ˆç”¨äºæŸå¤±è®¡ç®—ï¼‰
    
    å¢é‡é¢„æµ‹ä¼˜åŠ¿ï¼š
    - æ›´ç¨³å®šçš„é¢„æµ‹ï¼Œé¿å…å¤§å¹…åº¦è·³è·ƒ
    - ç¬¦åˆæ§åˆ¶ç³»ç»Ÿç›´è§‰ï¼ˆå°å¹…è°ƒæ•´ï¼‰
    - ç½‘ç»œå­¦ä¹ ç›¸å¯¹å˜åŒ–ï¼Œè€Œéç»å¯¹ä½ç½®
    - æŸå¤±è®¡ç®—é€»è¾‘ä¿æŒä¸å˜
    """
    
    def __init__(self, 
                 feature_dim=128,           # å•æ‰‹ç‰¹å¾ç»´åº¦
                 action_dim=3,              # è¾“å‡ºåŠ¨ä½œç»´åº¦ (dx, dy, dz)
                 lstm_hidden_dim=256,       # LSTMéšè—ç»´åº¦
                 lstm_num_layers=2,         # LSTMå±‚æ•°
                 dropout_rate=0.25,         # Dropoutæ¯”ç‡
                 pretrained_encoder_path=None,
                 action_embed_dim=64,       # åŠ¨ä½œåµŒå…¥ç»´åº¦
                 fc_hidden_dims=[128, 64],  # æœ€åå…¨è¿æ¥å±‚ç»´åº¦
                 ):
        """
        Args:
            feature_dim: å•æ‰‹è§¦è§‰ç‰¹å¾ç»´åº¦
            action_dim: è¾“å‡ºåŠ¨ä½œç»´åº¦
            lstm_hidden_dim: LSTMéšè—çŠ¶æ€ç»´åº¦
            lstm_num_layers: LSTMå±‚æ•°
            dropout_rate: Dropoutæ¯”ç‡
            pretrained_encoder_path: é¢„è®­ç»ƒç¼–ç å™¨æƒé‡è·¯å¾„
            action_embed_dim: åŠ¨ä½œåµŒå…¥ç»´åº¦
            fc_hidden_dims: æœ€åå…¨è¿æ¥å±‚çš„éšè—ç»´åº¦åˆ—è¡¨
        """
        super(TactilePolicyFeatureLSTM, self).__init__()
        
        self.feature_dim = feature_dim
        self.action_dim = action_dim
        self.lstm_hidden_dim = lstm_hidden_dim
        self.lstm_num_layers = lstm_num_layers
        self.action_embed_dim = action_embed_dim
        
        # åŠ è½½é¢„è®­ç»ƒçš„è§¦è§‰ç‰¹å¾æå–å™¨
        self.tactile_encoder = TactileCNNAutoencoder(
            in_channels=3, 
            latent_dim=feature_dim
        )
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if pretrained_encoder_path is not None and os.path.exists(pretrained_encoder_path):
            print(f"åŠ è½½é¢„è®­ç»ƒè§¦è§‰ç¼–ç å™¨: {pretrained_encoder_path}")
            checkpoint = torch.load(pretrained_encoder_path, map_location='cpu', weights_only=False)
            
            # æ£€æŸ¥checkpointæ ¼å¼ï¼Œæå–æ¨¡å‹çŠ¶æ€å­—å…¸
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    model_state = checkpoint['model_state_dict']
                    print("ğŸ“¦ æ£€æµ‹åˆ°è®­ç»ƒcheckpointæ ¼å¼ï¼Œæå–model_state_dict")
                elif 'state_dict' in checkpoint:
                    model_state = checkpoint['state_dict']
                    print("ğŸ“¦ æ£€æµ‹åˆ°state_dictæ ¼å¼")
                else:
                    model_state = checkpoint
                    print("ğŸ“¦ æ£€æµ‹åˆ°ç›´æ¥çŠ¶æ€å­—å…¸æ ¼å¼")
            else:
                model_state = checkpoint
            
            # åŠ è½½çŠ¶æ€å­—å…¸
            self.tactile_encoder.load_state_dict(model_state, strict=True)
            print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡")
            
            # æ‰“å°checkpointä¿¡æ¯
            if isinstance(checkpoint, dict) and 'epoch' in checkpoint:
                print(f"ğŸ“Š é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯: epoch {checkpoint['epoch']}")
                    
            
            # å†»ç»“ç‰¹å¾æå–å™¨å‚æ•°
            for param in self.tactile_encoder.parameters():
                param.requires_grad = False
            print("ğŸ”’ ç‰¹å¾æå–å™¨å‚æ•°å·²å†»ç»“")
        else:
            print("âŒ æ— æ³•å¯¼å…¥CNNç¼–ç å™¨")
            raise FileNotFoundError(f"é¢„è®­ç»ƒç¼–ç å™¨è·¯å¾„æ— æ•ˆ: {pretrained_encoder_path}")
        
        # åŠ¨ä½œåµŒå…¥ç¼–ç å™¨
        self.action_encoder = nn.Sequential(
            nn.Linear(action_dim, self.action_embed_dim),
            nn.LayerNorm(self.action_embed_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout_rate)
        )
        
        # LSTMè¾“å…¥ç»´åº¦: å·¦å³æ‰‹ç‰¹å¾ + åŠ¨ä½œåµŒå…¥
        lstm_input_dim = feature_dim * 2 + self.action_embed_dim  # 256 + 64 = 320
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=lstm_input_dim,
            hidden_size=lstm_hidden_dim,
            num_layers=lstm_num_layers,
            dropout=dropout_rate if lstm_num_layers > 1 else 0,
            batch_first=True  # è¾“å…¥æ ¼å¼: (batch, seq, feature)
        )
        
        # å…¨è¿æ¥å±‚
        fc_layers = []
        prev_dim = lstm_hidden_dim
        
        for hidden_dim in fc_hidden_dims:
            fc_layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        fc_layers.append(nn.Linear(prev_dim, action_dim))
        
        self.fc = nn.Sequential(*fc_layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params:,}, å¯è®­ç»ƒ {trainable_params:,}")
        
    def _initialize_weights(self):
        """åˆå§‹åŒ–LSTMæƒé‡"""
        for name, param in self.lstm.named_parameters():
            if 'weight' in name:
                if param.dim() >= 2:  # åªå¯¹2ç»´ä»¥ä¸Šçš„æƒé‡åº”ç”¨xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(param)
                else:
                    nn.init.normal_(param, 0, 0.01)  # å¯¹1ç»´æƒé‡ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
            elif 'bias' in name:
                nn.init.constant_(param, 0)
        
        # åˆå§‹åŒ–è¾“å‡ºå±‚æƒé‡
        for module in self.fc.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
    
    def forward(self, forces_l_seq, forces_r_seq, action_seq, seq_lengths=None, return_all_steps=False):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            forces_l_seq: å·¦æ‰‹è§¦è§‰åŠ›æ—¶åºæ•°æ® (B, T, 3, 20, 20)
            forces_r_seq: å³æ‰‹è§¦è§‰åŠ›æ—¶åºæ•°æ® (B, T, 3, 20, 20)
            action_seq: åŠ¨ä½œæ—¶åºæ•°æ® (B, T, 3)
            seq_lengths: æ¯ä¸ªåºåˆ—çš„å®é™…é•¿åº¦ (B,) å¯é€‰ï¼Œç”¨äºå˜é•¿åºåˆ—
            return_all_steps: æ˜¯å¦è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„é¢„æµ‹ (ç”¨äºå¯†é›†ç›‘ç£)
            
        Returns:
            å¦‚æœ return_all_steps=False: next_action (B, 3) - ä»…æœ€åæ—¶é—´æ­¥é¢„æµ‹
            å¦‚æœ return_all_steps=True: all_predictions (B, T, 3) - æ‰€æœ‰æ—¶é—´æ­¥é¢„æµ‹
        """
        batch_size, seq_len = forces_l_seq.size(0), forces_l_seq.size(1)
        
        # é‡å¡‘ä¸º (B*T, 3, 20, 20) ä»¥ä¾¿CNNå¤„ç†
        forces_l_flat = forces_l_seq.view(-1, 3, 20, 20)  # (B*T, 3, 20, 20)
        forces_r_flat = forces_r_seq.view(-1, 3, 20, 20)  # (B*T, 3, 20, 20)
        
        if self.tactile_encoder is not None:
            # ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨æå–ç‰¹å¾
            features_l_flat = self.tactile_encoder.encoder(forces_l_flat)  # (B*T, feature_dim)
            features_r_flat = self.tactile_encoder.encoder(forces_r_flat)  # (B*T, feature_dim)
        else:
            return None
        
        # é‡å¡‘å›æ—¶åºæ ¼å¼
        features_l_seq = features_l_flat.view(batch_size, seq_len, self.feature_dim)  # (B, T, feature_dim)
        features_r_seq = features_r_flat.view(batch_size, seq_len, self.feature_dim)  # (B, T, feature_dim)
        
        # åŠ¨ä½œåµŒå…¥
        action_embed_seq = self.action_encoder(action_seq)  # (B, T, action_embed_dim)
        
        # æ‹¼æ¥ç‰¹å¾: [features_l, features_r, action_embed]
        combined_features = torch.cat([
            features_l_seq, 
            features_r_seq, 
            action_embed_seq
        ], dim=-1)  # (B, T, 256 + action_embed_dim)
        
        # LSTMå¤„ç†æ—¶åºä¿¡æ¯
        if seq_lengths is not None:
            # å¤„ç†å˜é•¿åºåˆ—
            packed_input = nn.utils.rnn.pack_padded_sequence(
                combined_features, seq_lengths, batch_first=True, enforce_sorted=False
            )
            packed_output, (hidden, cell) = self.lstm(packed_input)
            lstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        else:
            # å›ºå®šé•¿åº¦åºåˆ—
            lstm_output, (hidden, cell) = self.lstm(combined_features)  # (B, T, lstm_hidden_dim)
        
        if return_all_steps:
            # è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„é¢„æµ‹ (ç”¨äºå¯†é›†ç›‘ç£)
            # é‡å¡‘LSTMè¾“å‡ºä»¥ä¾¿æ‰¹é‡å¤„ç† - ä½¿ç”¨reshapeè€Œä¸æ˜¯viewé¿å…å†…å­˜å¸ƒå±€é—®é¢˜
            lstm_flat = lstm_output.reshape(-1, self.lstm_hidden_dim)  # (B*T, lstm_hidden_dim)
            
            # é€šè¿‡å…¨è¿æ¥å±‚å¾—åˆ°æ‰€æœ‰æ—¶é—´æ­¥çš„å¢é‡é¢„æµ‹
            delta_predictions_flat = self.fc(lstm_flat)  # (B*T, action_dim) - å¢é‡é¢„æµ‹
            
            # é‡å¡‘å›æ—¶åºæ ¼å¼
            delta_predictions = delta_predictions_flat.reshape(batch_size, seq_len, self.action_dim)  # (B, T, action_dim)
            
            # å°†å¢é‡é¢„æµ‹è½¬æ¢ä¸ºç»å¯¹é¢„æµ‹
            # å¯¹äºç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä½¿ç”¨è¾“å…¥åŠ¨ä½œçš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ä½œä¸ºåŸºå‡†
            # å¯¹äºåç»­æ—¶é—´æ­¥ï¼Œä½¿ç”¨å‰ä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ä½œä¸ºåŸºå‡†
            all_predictions = torch.zeros_like(delta_predictions)
            
            for t in range(seq_len):
                if t == 0:
                    # ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼šé¢„æµ‹ = å½“å‰åŠ¨ä½œ + å¢é‡
                    all_predictions[:, t, :] = action_seq[:, t, :] + delta_predictions[:, t, :]
                else:
                    # åç»­æ—¶é—´æ­¥ï¼šé¢„æµ‹ = å‰ä¸€æ­¥é¢„æµ‹ + å¢é‡
                    all_predictions[:, t, :] = all_predictions[:, t-1, :] + delta_predictions[:, t, :]
            
            return all_predictions
        else:
            # ä»…è¿”å›æœ€åæ—¶é—´æ­¥çš„é¢„æµ‹ (åŸå§‹è¡Œä¸º)
            if seq_lengths is not None:
                # å–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆè¾“å‡º
                batch_indices = torch.arange(batch_size, device=lstm_output.device)
                last_outputs = lstm_output[batch_indices, seq_lengths - 1]  # (B, lstm_hidden_dim)
            else:
                last_outputs = lstm_output[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ (B, lstm_hidden_dim)
            
            # å…¨è¿æ¥å±‚é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»åŠ¨ä½œå¢é‡
            delta_action = self.fc(last_outputs)  # (B, action_dim) - é¢„æµ‹å¢é‡
            
            # å°†å¢é‡é¢„æµ‹è½¬æ¢ä¸ºç»å¯¹é¢„æµ‹ï¼ˆåŠ ä¸Šå½“å‰åŠ¨ä½œï¼‰
            # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„åŠ¨ä½œä½œä¸ºåŸºå‡†
            if seq_lengths is not None:
                batch_indices = torch.arange(batch_size, device=action_seq.device)
                current_action = action_seq[batch_indices, seq_lengths - 1]  # (B, 3)
            else:
                current_action = action_seq[:, -1, :]  # (B, 3)
            
            next_action = current_action + delta_action  # ç»å¯¹é¢„æµ‹ = å½“å‰åŠ¨ä½œ + å¢é‡
            
            return next_action
    
    def init_hidden(self, batch_size, device):
        """åˆå§‹åŒ–LSTMéšè—çŠ¶æ€"""
        hidden = torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_dim).to(device)
        cell = torch.zeros(self.lstm_num_layers, batch_size, self.lstm_hidden_dim).to(device)
        return (hidden, cell)


def compute_feature_lstm_losses(inputs, outputs, dataset=None, dense_supervision=False, current_epoch=1, total_epochs=100):
    """
    è®¡ç®—Feature-LSTMæŸå¤± - æ”¯æŒå¯†é›†ç›‘ç£å’Œæ—¶åºé¢„æµ‹ï¼ŒåŒ…å«æ­¥é•¿èŒƒæ•°æƒé‡å’Œæ–¹å‘ä¸€è‡´æ€§æŸå¤±
    
    Args:
        inputs: è¾“å…¥æ•°æ®å­—å…¸ï¼ŒåŒ…å« 'target_next_action' æˆ– 'target_action_seq'
        outputs: æ¨¡å‹è¾“å‡ºå¼ é‡ 
                 å¦‚æœdense_supervision=False: (B, 3) - ä¸‹ä¸€æ—¶åˆ»é¢„æµ‹åŠ¨ä½œ
                 å¦‚æœdense_supervision=True: (B, T, 3) - æ‰€æœ‰æ—¶é—´æ­¥é¢„æµ‹åŠ¨ä½œ
        dataset: æ•°æ®é›†å¯¹è±¡ï¼Œç”¨äºåå½’ä¸€åŒ–è®¡ç®—çœŸå®æŸå¤±
        dense_supervision: æ˜¯å¦ä½¿ç”¨å¯†é›†ç›‘ç£ (æ¯ä¸ªæ—¶é—´æ­¥éƒ½è®¡ç®—æŸå¤±)
        current_epoch: å½“å‰è®­ç»ƒè½®æ•° (ä»1å¼€å§‹)
        total_epochs: æ€»è®­ç»ƒè½®æ•°
        
    Returns:
        loss: æ€»æŸå¤±
        metrics: æŸå¤±åˆ†è§£å­—å…¸
    """
    if dense_supervision:
        # å¯†é›†ç›‘ç£æ¨¡å¼ï¼šå¯¹æ‰€æœ‰æ—¶é—´æ­¥è®¡ç®—æŸå¤±
        predicted_action_seq = outputs  # (B, T, 3)
        target_action_seq = inputs['target_action_seq']  # (B, T, 3)
        
        # è®¡ç®—æ­¥é•¿èŒƒæ•° (B, T)
        predicted_step_norms = torch.norm(predicted_action_seq, dim=-1)  # (B, T)
        target_step_norms = torch.norm(target_action_seq, dim=-1)  # (B, T)
        
        # è®¡ç®—å¹³å‡æ­¥é•¿ï¼ˆç”¨äºé˜ˆå€¼åˆ¤æ–­ï¼‰
        avg_step_norm = target_step_norms.mean()
        
        # æ­¥é•¿èŒƒæ•°æƒé‡ï¼šé¼“åŠ±é¢„æµ‹å¤§æ­¥é•¿ï¼Œæƒ©ç½šé¢„æµ‹å°æ­¥é•¿
        step_weights = torch.ones_like(target_step_norms)
        large_step_mask = target_step_norms > avg_step_norm
        small_step_mask = target_step_norms < avg_step_norm * 0.3  # éå¸¸å°çš„æ­¥é•¿
        step_weights[large_step_mask] = 2.0  # å¤§æ­¥é•¿ç»™äºˆ2å€æƒé‡ï¼ˆé‡è¦çš„ç§»åŠ¨ï¼‰
        # å¯¹äºå°æ­¥é•¿ï¼Œæˆ‘ä»¬ç»™äºˆå°æƒé‡è€Œä¸æ˜¯å¤§æƒ©ç½šï¼Œé¿å…ç½‘ç»œä¸“æ³¨äºé¢„æµ‹å°å€¼
        step_weights[small_step_mask] = 0.1  # å°æ­¥é•¿ç»™äºˆè¾ƒå°æƒé‡
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„åŸºç¡€æŸå¤±
        l1_loss = F.l1_loss(predicted_action_seq, target_action_seq, reduction='none').mean(dim=-1)  # (B, T)
        mse_loss = F.mse_loss(predicted_action_seq, target_action_seq, reduction='none').mean(dim=-1)  # (B, T)
        
        # åº”ç”¨æ­¥é•¿æƒé‡
        weighted_l1_loss = l1_loss * step_weights
        weighted_mse_loss = mse_loss * step_weights
        
        # æ–¹å‘ä¸€è‡´æ€§æŸå¤±ï¼ˆè§’åº¦æŸå¤±ï¼š1 - cos(Î¸_pred, Î¸_target)ï¼‰
        direction_loss = torch.tensor(0.0, device=predicted_action_seq.device)
        if predicted_action_seq.size(1) > 0:  # è‡³å°‘æœ‰1ä¸ªæ—¶é—´æ­¥
            # è®¡ç®—é¢„æµ‹åŠ¨ä½œå’Œç›®æ ‡åŠ¨ä½œçš„æ–¹å‘å‘é‡ï¼ˆå½’ä¸€åŒ–ï¼‰
            pred_norms = torch.norm(predicted_action_seq, dim=-1, keepdim=True)  # (B, T, 1)
            target_norms = torch.norm(target_action_seq, dim=-1, keepdim=True)  # (B, T, 1)
            
            # åˆ›å»ºæœ‰æ•ˆæ©ç ï¼ˆæ’é™¤é›¶å‘é‡ï¼‰  å™ªå£°æ»¤æ³¢å¸¸é‡
            valid_mask = (pred_norms.squeeze(-1) > 1e-6) & (target_norms.squeeze(-1) > 1e-6)  # (B, T)
            
            if valid_mask.sum() > 0:
                # å½’ä¸€åŒ–æ–¹å‘å‘é‡
                pred_directions = predicted_action_seq / (pred_norms + 1e-8)  # (B, T, 3)
                target_directions = target_action_seq / (target_norms + 1e-8)  # (B, T, 3)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cosine_sim = (pred_directions * target_directions).sum(dim=-1)  # (B, T)
                cosine_sim = torch.clamp(cosine_sim, min=-1.0, max=1.0)  # ç¡®ä¿åœ¨[-1,1]èŒƒå›´å†…
                
                # è§’åº¦æŸå¤±ï¼š1 - cos(Î¸)ï¼Œåªè®¡ç®—æœ‰æ•ˆå‘é‡çš„æŸå¤±
                angle_losses = 1.0 - cosine_sim  # (B, T)
                
                # ä¸ºè§’åº¦æŸå¤±æ·»åŠ æ­¥é•¿æƒé‡ï¼šå°æ­¥é•¿çš„è§’åº¦æŸå¤±æƒé‡è¾ƒå°
                angle_weights = torch.ones_like(target_step_norms)
                small_angle_mask = target_step_norms < avg_step_norm * 0.5  # å°äºå¹³å‡æ­¥é•¿çš„0.5å€
                angle_weights[small_angle_mask] = 0.3  # å°æ­¥é•¿è§’åº¦æŸå¤±ç»™äºˆæ›´å°æƒé‡
                
                # åº”ç”¨è§’åº¦æƒé‡ï¼Œåªå¯¹æœ‰æ•ˆæ©ç å†…çš„æŸå¤±è¿›è¡ŒåŠ æƒå¹³å‡
                weighted_angle_losses = angle_losses * angle_weights
                direction_loss = weighted_angle_losses[valid_mask].mean()
        
        # æ·»åŠ å¹…åº¦æŸå¤±ï¼šæƒ©ç½šé¢„æµ‹å¹…åº¦ä¸ç›®æ ‡å¹…åº¦å·®å¼‚è¿‡å¤§
        magnitude_loss = F.mse_loss(predicted_step_norms, target_step_norms, reduction='mean')
        
        # æ·»åŠ è¾“å‡ºå¤šæ ·æ€§æŸå¤±ï¼šé¼“åŠ±ç½‘ç»œè¾“å‡ºæ›´å¤šæ ·åŒ–çš„å¢é‡ï¼Œé˜²æ­¢è¾“å‡ºè¿‡äºå•ä¸€
        diversity_loss = torch.tensor(0.0, device=predicted_action_seq.device)
        if predicted_action_seq.size(0) > 1:  # æ‰¹æ¬¡å¤§å° > 1
            # è®¡ç®—æ‰¹æ¬¡å†…é¢„æµ‹çš„æ ‡å‡†å·®ï¼Œé¼“åŠ±ä¸åŒæ ·æœ¬æœ‰ä¸åŒçš„é¢„æµ‹
            pred_std = torch.std(predicted_action_seq, dim=0).mean()  # (T, 3) -> scalar
            # å¦‚æœæ ‡å‡†å·®å¤ªå°ï¼Œè¯´æ˜æ‰€æœ‰é¢„æµ‹éƒ½å¾ˆç›¸ä¼¼ï¼Œç»™äºˆæƒ©ç½š
            # ä½¿ç”¨ exp(-alpha * std) ä¿æŒå€¼åœ¨ (0,1]ï¼Œå¹¶æ˜¾å¼é™åˆ¶åˆ° [0,1] ä»¥é˜²æ•°å€¼å¼‚å¸¸
            diversity_loss = torch.exp(-pred_std * 10.0)
            diversity_loss = torch.clamp(diversity_loss, min=0.0, max=1.0)
        
        # è®¡ç®—diversity_lossçš„åˆ†æ®µè¡°å‡ç³»æ•°
        # å‰50%å›åˆï¼šç³»æ•°ä¿æŒ0.1ä¸å˜
        # å50%å›åˆï¼šç³»æ•°ä»0.1çº¿æ€§è¡°å‡åˆ°0.01
        halfway_epoch = total_epochs // 2
        if current_epoch <= halfway_epoch:
            # å‰50%å›åˆï¼Œç³»æ•°ä¿æŒ0.1
            diversity_weight = 0.1
        else:
            # å50%å›åˆï¼Œçº¿æ€§è¡°å‡ 0.1 -> 0.01
            progress = (current_epoch - halfway_epoch) / (total_epochs - halfway_epoch)
            diversity_weight = 0.1 - (0.1 - 0.0) * progress  # ä»0.1è¡°å‡
        
        # è®¡ç®—æ€»æŸå¤±
        base_loss = 0.5 * weighted_l1_loss.mean() + 0.5 * weighted_mse_loss.mean()
        total_loss = 1.0 * base_loss + 1.0 * direction_loss + 0.0 * magnitude_loss + diversity_weight * diversity_loss
        
        
        # è®¡ç®—æ ‡é‡æŸå¤±ç”¨äºè®°å½•
        l1_loss_scalar = l1_loss.mean()
        mse_loss_scalar = mse_loss.mean()
        
        # è¯„ä¼°æŒ‡æ ‡
        with torch.no_grad():
            rmse_loss = torch.sqrt(mse_loss_scalar)
            step_norm_penalty = (step_weights - 1.0).mean()  # å¹³å‡æ­¥é•¿æƒ©ç½š
            
            # è®¡ç®—çœŸå®æŸå¤±ï¼ˆåå½’ä¸€åŒ–åçš„L1æŸå¤±ï¼‰
            real_l1_loss = 0.0
            real_l1_loss_max = 0.0
            if dataset is not None and hasattr(dataset, 'denormalize_data'):
                try:
                    # åå½’ä¸€åŒ–é¢„æµ‹å€¼å’Œç›®æ ‡å€¼ (éœ€è¦é‡å¡‘ä¸ºæ‰¹æ¬¡å¤„ç†)
                    B, T, _ = predicted_action_seq.shape
                    pred_flat = predicted_action_seq.view(-1, 3).detach().cpu().numpy()
                    target_flat = target_action_seq.view(-1, 3).detach().cpu().numpy()
                    
                    pred_denorm = dataset.denormalize_data(pred_flat, 'actions')
                    target_denorm = dataset.denormalize_data(target_flat, 'actions')
                    
                    # è®¡ç®—é€æ ·æœ¬çš„çœŸå®L1æŸå¤±
                    sample_real_l1_losses = np.mean(np.abs(pred_denorm - target_denorm), axis=1)  # (B*T,)
                    
                    # è®¡ç®—å¹³å‡å€¼å’Œæœ€å¤§å€¼
                    real_l1_loss = np.mean(sample_real_l1_losses)
                    real_l1_loss_max = np.max(sample_real_l1_losses)
                except Exception as e:
                    print(f"âš ï¸  è®¡ç®—çœŸå®æŸå¤±å¤±è´¥: {e}")
                    real_l1_loss = 0.0
                    real_l1_loss_max = 0.0
    else:
        # åŸå§‹æ¨¡å¼ï¼šä»…å¯¹æœ€åæ—¶é—´æ­¥è®¡ç®—æŸå¤±ï¼ˆå¢é‡é¢„æµ‹ï¼‰
        return TypeError("Invalid supervision mode")
        
    # è¿”å›çš„æ˜¯æœªä½œåŠ æƒçš„æŸå¤±å€¼
    metrics = {
        'train_loss': total_loss.item(),
        'l1_error': l1_loss_scalar.item(),
        'mse_error': mse_loss_scalar.item(),
        'rmse_error': rmse_loss.item(),
        'direction_loss': direction_loss.item(),
        'magnitude_loss': magnitude_loss.item(),
        'diversity_loss': diversity_loss.item(),
        'diversity_weight': diversity_weight,  # è®°å½•å½“å‰ä½¿ç”¨çš„diversityæƒé‡
        'step_norm_penalty': step_norm_penalty.item(),
        'avg_step_norm': avg_step_norm.item(),
        'real_l1_error(mm)': real_l1_loss * 1000,  # çœŸå®æŸå¤±ï¼ˆåå½’ä¸€åŒ–åï¼‰
        'real_l1_error_max(mm)': real_l1_loss_max * 1000,  # æ¯ä¸ªbatchä¸­çš„æœ€å¤§çœŸå®æŸå¤±
    }
    
    return total_loss, metrics


def prepare_feature_lstm_input_from_sequence_dataset(batch_data, dense_supervision=False):
    """
    ä»SequenceDatasetæ‰¹æ¬¡ä¸­å‡†å¤‡Feature-LSTMæ¨¡å‹çš„è¾“å…¥ - æ”¯æŒå¯†é›†ç›‘ç£
    
    Args:
        batch_data: æ¥è‡ªSequenceDatasetçš„æ‰¹æ¬¡æ•°æ®
        dense_supervision: æ˜¯å¦ä½¿ç”¨å¯†é›†ç›‘ç£
    
    Returns:
        dict: Feature-LSTMæ¨¡å‹çš„è¾“å…¥å­—å…¸
    """
    inputs = {
        'forces_l': batch_data['forces_l_seq'],      # (B, T, 3, 20, 20)
        'forces_r': batch_data['forces_r_seq'],      # (B, T, 3, 20, 20)
        'actions': batch_data['action_seq'],          # (B, T, 3)
        'seq_lengths': batch_data.get('seq_lengths', None)      # (B,) å¯é€‰
    }
    
    if dense_supervision:
        # å¯†é›†ç›‘ç£ï¼šæä¾›æ‰€æœ‰æ—¶é—´æ­¥çš„ç›®æ ‡åŠ¨ä½œåºåˆ—
        inputs['target_action_seq'] = batch_data['target_action_seq']  # (B, T, 3)
    else:
        # ç¨€ç–ç›‘ç£ï¼šåªæä¾›æœ€åæ—¶é—´æ­¥çš„ç›®æ ‡åŠ¨ä½œï¼Œä»¥åŠå½“å‰åŠ¨ä½œç”¨äºå¢é‡é¢„æµ‹
        inputs['target_next_action'] = batch_data['target_next_action']  # (B, 3)
        # æä¾›å½“å‰åŠ¨ä½œï¼ˆåºåˆ—çš„æœ€åä¸€ä¸ªåŠ¨ä½œï¼‰ç”¨äºå¢é‡é¢„æµ‹
        inputs['current_action'] = batch_data['action_seq'][:, -1, :]  # (B, 3)
    
    return inputs


def create_tactile_policy_feature_lstm(config):
    """åˆ›å»ºè§¦è§‰ç­–ç•¥Feature-LSTMæ¨¡å‹"""
    return TactilePolicyFeatureLSTM(
        feature_dim=config.get('feature_dim', 128),
        action_dim=config.get('action_dim', 3),
        lstm_hidden_dim=config.get('lstm_hidden_dim', 256),
        lstm_num_layers=config.get('lstm_num_layers', 2),
        dropout_rate=config.get('dropout_rate', 0.25),
        pretrained_encoder_path=config.get('pretrained_encoder_path', None),
        action_embed_dim=config.get('action_embed_dim', 64),
        fc_hidden_dims=config.get('fc_hidden_dims', [128, 64])
    )


if __name__ == '__main__':
    # ç®€å•æµ‹è¯•
    config = {
        'feature_dim': 128,
        'action_dim': 3,
        'lstm_hidden_dim': 256,
        'lstm_num_layers': 2,
        'dropout_rate': 0.1,
        'pretrained_encoder_path': None,
        'action_embed_dim': 64,
        'fc_hidden_dims': [128, 64]
    }
    
    model = create_tactile_policy_feature_lstm(config)
    
    # æµ‹è¯•æ—¶åºè¾“å…¥
    batch_size, seq_len = 4, 10
    forces_l_seq = torch.randn(batch_size, seq_len, 3, 20, 20)
    forces_r_seq = torch.randn(batch_size, seq_len, 3, 20, 20)
    action_seq = torch.randn(batch_size, seq_len, 3)
    
    output = model(forces_l_seq, forces_r_seq, action_seq)
    
    print(f"è¾“å…¥è§¦è§‰åŠ›låºåˆ—å½¢çŠ¶: {forces_l_seq.shape}")
    print(f"è¾“å…¥è§¦è§‰åŠ›råºåˆ—å½¢çŠ¶: {forces_r_seq.shape}")
    print(f"è¾“å…¥åŠ¨ä½œåºåˆ—å½¢çŠ¶: {action_seq.shape}")
    print(f"è¾“å‡ºä¸‹ä¸€åŠ¨ä½œå½¢çŠ¶: {output.shape}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # æµ‹è¯•æŸå¤±è®¡ç®—
    inputs = {'target_next_action': torch.randn_like(output)}
    loss, metrics = compute_feature_lstm_losses(inputs, output)
    
    print(f"æ€»æŸå¤±: {loss.item():.4f}")
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            print(f"  {key}: {value:.4f}")
    
    print("âœ… Feature-LSTMæ¨¡å‹æµ‹è¯•å®Œæˆï¼")
